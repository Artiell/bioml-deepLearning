# ğŸ”¢ Projet Deep Learning â€“ Classification MNIST (MLP, Deep NN, CNN)

**UniversitÃ© Claude Bernard Lyon 1 â€” 2025**  
**Master 2 Intelligence Artificielle**  
**UE : Introduction au Deep Learning**

**Auteurs :**  
- **Youssef Abida**  
- **Nathan Corroller**

Ce projet a Ã©tÃ© rÃ©alisÃ© dans un **cadre universitaire** afin de mettre en Å“uvre diffÃ©rents modÃ¨les profonds en PyTorch : Perceptron, rÃ©seau peu profond (Shallow), rÃ©seau profond (Deep Network) et rÃ©seau de neurones convolutionnel (CNN).  
Il repose sur le dataset **MNIST**, composÃ© dâ€™images 28Ã—28 de chiffres manuscrits.

---

## ğŸ¯ 1. Objectifs du projet

Le but du projet est de :

- ImplÃ©menter et comprendre les architectures MLP, Deep MLP et CNN.  
- Se familiariser avec **PyTorch** : tenseurs, gradients automatiques, DataLoader, modules, optimizers.  
- ExpÃ©rimenter diffÃ©rents **couples optimiseur / fonction de perte**.  
- Ã‰tudier lâ€™influence des **hyperparamÃ¨tres**.  
- Comparer les rÃ©sultats obtenus entre architectures.  

Ce travail reprend les consignes du document pÃ©dagogique fourni â€• notamment les parties :

1. Perceptron  
2. Shallow network  
3. Deep network  
4. CNN  

---

## ğŸ—‚ï¸ 2. Le dataset MNIST

- Images : **28Ã—28**, aplaties en vecteurs de dimension **784**  
- Labels : encodÃ©s en **one-hot**  
- Structure des donnÃ©es : (train_images, train_labels), (test_images, test_labels)


Les labels reprÃ©sentent les chiffres 0 Ã  9.

---

## ğŸ§© 3. Partie 1 â€“ Perceptron

Cette premiÃ¨re partie introduit les tenseurs et la mise Ã  jour des poids.

### ğŸ“Œ Description des tenseurs manipulÃ©s

| Nom | Taille | Description |
|------|---------|-------------|
| data_train | (63000, 784) | Images dâ€™entraÃ®nement |
| label_train | (63000, 10) | Labels one-hot |
| data_test | (7000, 784) | Images de test |
| label_test | (7000, 10) | Labels test |
| w | (784, 10) | Poids du perceptron |
| b | (1, 10) | Biais |
| x | (batch_size, 784) | Batch dâ€™images |
| y | (batch_size, 10) | PrÃ©dictions |
| t | (batch_size, 10) | Labels cibles |
| grad | (batch_size, 10) | Gradient dâ€™erreur |

### ğŸ“˜ RÃ¨gle dâ€™apprentissage

w â† w + Î· * Xáµ€ * (t â€“ y)
b â† b + Î· * sum(t â€“ y)


Cette section permet de comprendre en profondeur la propagation avant et arriÃ¨re dans PyTorch.

---

## ğŸŒ± 4. Partie 2 â€“ Shallow Network (1 couche cachÃ©e)

### ğŸ§ª MÃ©thodologie

- Une classe `ShallowNetwork` a Ã©tÃ© dÃ©veloppÃ©e.  
- Un ensemble **validation (10 %)** a Ã©tÃ© crÃ©Ã© pour Ã©viter lâ€™overfitting.  
- Lâ€™entraÃ®nement suit les Ã©tapes classiques :  
  mÃ©lange des donnÃ©es, dÃ©coupage en batchs, propagation, perte, backprop, mise Ã  jour.  

### ğŸ” GridSearch 1 â€” SGD + MSELoss  

ParamÃ¨tres testÃ©s :

- Î· âˆˆ {0.08, 0.3}  
- batch_size âˆˆ {10, 30, 64}  
- hidden_size âˆˆ {512, 768, 1024}  
- epochs âˆˆ {20, 30}  

### ğŸ” GridSearch 2 â€” Adam + CrossEntropyLoss  

ParamÃ¨tres testÃ©s :

- Î· âˆˆ {0.001, 0.0008}  
- batch_size âˆˆ {32, 64}  
- hidden_size âˆˆ {512, 768}  
- epochs âˆˆ {20, 25}  

### ğŸ“Š RÃ©sultats

- Les deux approches donnent de **trÃ¨s bonnes performances**.  
- SGD + MSE fonctionne bien car les labels sont en one-hot.
- Adam + CrossEntropy converge plus vite mais demande un rÃ©glage plus prÃ©cis.

**Accuracy obtenue : ~98 % sur le test.**

---

## ğŸ§± 5. Partie 3 â€“ Deep Network (MLP profond)

### ğŸ›ï¸ ExpÃ©rimentation

4 configurations ont Ã©tÃ© testÃ©es :

| Session | Optimizer | Loss | Objectif |
|---------|-----------|------|-----------|
| GS1 | SGD | MSE | Baseline cohÃ©rente avec le shallow |
| GS2 | SGD | CrossEntropy | Tester lâ€™effet du softmax implicite |
| GS3 | Adam | MSE | Tester compatibilitÃ© Adam + MSE |
| GS4 | Adam | CrossEntropy | Combinaison la plus courante |

### ğŸ”¢ HyperparamÃ¨tres explorÃ©s

- Î· âˆˆ {0.0008, 0.001, 0.01}  
- batch_size âˆˆ {32, 64, 128}  
- architectures :  
  - [512, 256, 128]  
  - [1024, 768, 512, 256, 128]  
- epochs âˆˆ {20, 30}  

### ğŸ“Š RÃ©sultats remarquables

- **SGD + MSE :** stable mais plus lent  
- **SGD + CE :** bonne convergence, learning rate sensible  
- **Adam + MSE :** surprisingly efficient  
- **Adam + CE :** meilleures performances globales  

**Accuracy max : ~96.8 %.**

---

## ğŸ§  6. Partie 4 â€“ Convolutional Neural Network (CNN)

Le CNN est le modÃ¨le le plus performant, exploitant la structure spatiale des images.

### ğŸ—ï¸ Architecture finale

1. Conv2d(1 â†’ 32, kernel=3, stride=1, padding=1) + ReLU  
2. Conv2d(32 â†’ 64, kernel=3, stride=1, padding=1) + ReLU  
3. MaxPool2d(2Ã—2)  
4. Dropout(0.25)  
5. Fully connected 64Ã—14Ã—14 â†’ 128 + ReLU  
6. Dropout(0.25)  
7. Fully connected 128 â†’ 10  

### âš™ï¸ HyperparamÃ¨tres finaux

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Learning rate | 0.001 |
| Batch size | 64 |
| Epochs | 9 |
| Optimizer | Adam |
| Loss | CrossEntropyLoss |

### ğŸ“ˆ Performances

| Jeu | Loss | Accuracy |
|-----|------|-----------|
| EntraÃ®nement | â‰ˆ 0.0156 | 99.41 % |
| Validation | â‰ˆ 0.0364 | 99.10 % |
| Test | â‰ˆ 0.0395 | **99.29 %** |

Le CNN obtient les **meilleurs scores** du projet.

---

## ğŸ¥‡ 7. Comparaison globale des modÃ¨les

| ModÃ¨le | Acc. Train | Acc. Test | Commentaire |
|--------|------------|------------|--------------|
| Perceptron | 90 % | 85 % | LinÃ©aire, trÃ¨s simple |
| Shallow (SGD/MSE) | 99.5 % | 98.6 % | TrÃ¨s performant |
| Shallow (Adam/CE) | 99.1 % | 98.5 % | Convergence plus rapide |
| Deep (SGD/MSE) | 93.9 % | 93.8 % | Sous-apprentissage |
| Deep (Adam/CE) | 99 % | 98.5 % | Meilleure stabilitÃ© |
| CNN | 99.2 % | **98.8 %** | ğŸ† Meilleur modÃ¨le |

---

## ğŸ”® 8. Perspectives dâ€™amÃ©lioration

Deux pistes envisagÃ©es mais non implÃ©mentÃ©es :

### 1. Early Stopping  
- DÃ©tection automatique de stagnation de la validation.  
- Ã‰vite le surapprentissage.

### 2. Optimisation bayÃ©sienne  
- Recherche automatique des hyperparamÃ¨tres.  
- Plus efficace quâ€™une gridsearch exhaustive.

---

## ğŸ Conclusion

Ce projet universitaire nous a permis de :

- comprendre les mÃ©canismes internes des rÃ©seaux de neurones,  
- maÃ®triser les outils PyTorch,  
- analyser lâ€™influence des hyperparamÃ¨tres,  
- comparer diffÃ©rentes architectures (MLP vs CNN),  
- constater lâ€™efficacitÃ© des CNN pour les images.

Le travail rÃ©alisÃ© montre une progression logique :  
**Perceptron â†’ Shallow â†’ Deep â†’ CNN**,  
avec une montÃ©e en complexitÃ© et en performance.

---


